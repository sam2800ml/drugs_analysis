# ETL Pipeline with Apache Airflow, Great Expectations, and Docker

## üìå Project Overview

This repository contains an ETL pipeline built using Apache Airflow for orchestrating workflows, Great Expectations for data quality validation, and Docker for containerizing the process. The project demonstrates how to extract, transform, and load (ETL) data efficiently while ensuring data integrity.

‚îú‚îÄ‚îÄ dags/                                 # Airflow DAGs for orchestrating ETL
|   ‚îú‚îÄ‚îÄ tasks/                            # This is where all the task are
|   |    ‚îú‚îÄ‚îÄ‚îÄ create_table.py             # Creation of the first drug_database
|   |    ‚îú‚îÄ‚îÄ‚îÄ create_transformtable.py    # Creation of the transforms tables tdate_db, transformed_db
|   |    ‚îú‚îÄ‚îÄ‚îÄ datatransform_load.py       # Loading the transform data into the corresponded tables
|   |    ‚îú‚îÄ‚îÄ‚îÄ load.py                     # Calling the dataset
|   |    ‚îú‚îÄ‚îÄ‚îÄ test_loading.py             # Loading the dataset into the first table 
|   |    ‚îú‚îÄ‚îÄ‚îÄ Transform.py                # All the transformations applied
|   ‚îú‚îÄ‚îÄ main.py                           # This isthe creation of the dags
‚îú‚îÄ‚îÄ notebooks/                            # Jupyter notebooks for EDA
‚îÇ   ‚îú‚îÄ‚îÄ eda_before.ipynb                  # EDA before transformations
‚îÇ   ‚îú‚îÄ‚îÄ eda_after.ipynb                   # EDA after transformations
‚îú‚îÄ‚îÄ gx/                                   # Great Expectations configurations
‚îÇ   ‚îú‚îÄ‚îÄ expectations/                     # Data validation rules
|   |    ‚îú‚îÄ‚îÄ‚îÄ drugs_database_suite.json   # Expectations for the first table
|   |    ‚îú‚îÄ‚îÄ‚îÄ expectations_date.json      # Expectations for the date table
|   |    ‚îú‚îÄ‚îÄ‚îÄ expectations_transform.json # Expectation for the transform table
‚îú‚îÄ‚îÄ docker-compose.yml                    # Docker setup for the project
‚îú‚îÄ‚îÄ Dockerfile                            # Docker setup for the project
‚îú‚îÄ‚îÄ requirements.txt                      # Python dependencies
‚îî‚îÄ‚îÄ README.md                             # Project documentation
## Prerequisites

Before running the project, ensure you have:

- Docker installed and running
- Git installed on your machine

## 1. Clone the Project Repository

Open a terminal and run:

```bash
git clone https://github.com/sam2800ml/drugs_analysis.git
cd drugs_analysis
```

## 2. Start Docker and Build the Containers

Make sure Docker is running, then execute:

```bash
docker compose build
echo -e "AIRFLOW_UID=$(id -u)" > .env
docker compose up -d
```

## 3. Get the PostgreSQL Container IP

First, list running Docker containers:

```bash
docker ps
```

Find the **container ID** of the PostgreSQL image and inspect it:

```bash
docker inspect <container_id>
```

Search for the `"IPAddress"` field and copy the IP address.

## 4. Access Apache Airflow

Open your browser and go to:

```
http://localhost:8080
```

Log in with:

- **Username**: `airflow`
- **Password**: `airflow`

## 5. Configure Airflow Connection

1. Click the **Admin** dropdown at the top of the Airflow UI.
2. Select **Connections**.
3. Click the **‚ûï (plus)** button to add a new connection.
4. Fill in the following details:
   - **Connection ID**: `postgres_database`
   - **Connection Type**: `Postgres`
   - **Host**: *(Use the IP address of the PostgreSQL container from Step 3)*
   - **Database**: `airflow`
   - **Login**: `airflow`
   - **Password**: `airflow`
   - **Port**: `5432`
5. Click **Save**.

## 6. Run the DAG

1. In Airflow, navigate to the **DAGs** page.
2. Locate your DAG and click on its name.
3. Click the **Play ‚ñ∂Ô∏è button** in the top-right corner to trigger it manually.

---

### üéâ Your project is now set up and running! üöÄ

